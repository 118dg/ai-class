{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tensorflow.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOOs4+SPNKY05vLCQytlnkv"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zZFt1CjQC5i-"},"source":["## Tensorflow Intro\n","\n","텐서플로(TensorFlow)는 머신러닝 프로그램, 특히 딥러닝 프로그램을 쉽게 구현할 수 있도록 다양한 기능을 제공해주는 머신러닝 라이브러리로, 모두가 잘 아는 구글에서 만들었습니다.\n","\n","텐서플로 자체는 기본적으로 C++로 작성했지만 파이썬, 자바, 고(Go) 등 다양한 언어를 지원합니다. 다만, 파이썬을 최우선으로 지원하여 대다수 편의 기능이 파이썬 라이브러리로만 구현되어 있으니 되도록이면 파이썬으로 개발하는 것이 가장 편리합니다.\n","\n","또한 윈도우, 맥, 리눅스뿐만 아니라 안드로이드, iOS, 라즈베리 파이 등 다양한 시스템에서 쉽게 사용할 수 있도록 지원하여 매우 다양한 곳에 응용할 수 있습니다.\n","\n","물론 머신러닝/딥러닝을 위한 라이브러리로 텐서플로가 유일한 것은 아닙니다. 토치(Torch), 카페(Caffe), MXNet, 체이너(Chainer), CNTK 등 많은 라이브러리가 있습니다. 그렇다면 왜 텐서플로를 사용하는 것일까요? 제가 생각하는 답은 커뮤니티입니다. 특히 저 같은 엔지니어에게 있어서 라이브러리를 선택할 때 가장 중요한 기준은 커뮤니티라고 생각합니다. 실무에 적용했을 때 생기는 문제점들을 해결하거나, 라이브러리 자체에 버그가 있을 때 얼마나 빠르게 수정되는가 하는 그런 것들. 바로 그런 요인들이 실무를 하는 엔지니어에게는 가장 중요한 부분이라고 할 수 있을 것입니다.\n","\n","그런 점에 있어 현존하는 머신러닝 라이브러리 중 커뮤니티가 가장 북적이는 것이 바로 텐서플로입니다. 깃허브의 텐서플로 저장소나 각종 애플리케이션, 클라우드 서비스 등은 물론, 새로운 논문이 나올 때마다 텐서플로로 된 구현체가 가장 먼저 나올 정도로 텐서플로 커뮤니티는 놀라울 만큼 활발하게 움직이고 있습니다."]},{"cell_type":"markdown","metadata":{"id":"SAKjF0pIE4Qa"},"source":["# import, constant, session"]},{"cell_type":"code","metadata":{"id":"9KuuHf-jCuQp","executionInfo":{"status":"ok","timestamp":1603201223716,"user_tz":-540,"elapsed":3060,"user":{"displayName":"네모박스","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHcCl8G2pIAYWJnTlHiJMoKULVcpaM5oyN90l4iQ=s64","userId":"03122266989642426519"}},"outputId":"599cb948-d4a8-4c59-98d4-a35f31c9af92","colab":{"base_uri":"https://localhost:8080/","height":93}},"source":["# 텐서플로우 import\n","import tensorflow as tf\n","\n","# v1 호환성\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior() "],"execution_count":1,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XPKFRgJTDLaj","executionInfo":{"status":"ok","timestamp":1603201223717,"user_tz":-540,"elapsed":3049,"user":{"displayName":"네모박스","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHcCl8G2pIAYWJnTlHiJMoKULVcpaM5oyN90l4iQ=s64","userId":"03122266989642426519"}},"outputId":"3aa0b390-fc14-4018-aaf2-66d6c5ffe997","colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["# tf.constant: 말 그대로 상수\n","hello = tf.constant('Hello, TensorFlow!')\n","print(hello)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Tensor(\"Const:0\", shape=(), dtype=string)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HXf737tOEdbz","executionInfo":{"status":"ok","timestamp":1603201223717,"user_tz":-540,"elapsed":3042,"user":{"displayName":"네모박스","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHcCl8G2pIAYWJnTlHiJMoKULVcpaM5oyN90l4iQ=s64","userId":"03122266989642426519"}},"outputId":"e443677a-c4b2-4e8b-bd6e-75e05e78ca41","colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["a = tf.constant(10)\n","b = tf.constant(32)\n","c = tf.add(a, b)  # a + b 로도 쓸 수 있음\n","print(c)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Tensor(\"Add:0\", shape=(), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RufJq9EYEiOn","executionInfo":{"status":"ok","timestamp":1603201223717,"user_tz":-540,"elapsed":3041,"user":{"displayName":"네모박스","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHcCl8G2pIAYWJnTlHiJMoKULVcpaM5oyN90l4iQ=s64","userId":"03122266989642426519"}}},"source":["# 위에서 변수와 수식들을 정의했지만, 실행이 정의한 시점에서 텐서플로우 학습이 실행되는 것은 아닙니다.\n","# 아래처럼 Session 객체와 run 메소드를 사용할 때 계산이 됩니다.\n","\n","# 따라서 모델을 구성하는 단계, 실행하는 단계를 분리하여 프로그램을 깔끔하게 작성할 수 있습니다."],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"yzeFuZasEt48","executionInfo":{"status":"ok","timestamp":1603201223718,"user_tz":-540,"elapsed":3036,"user":{"displayName":"네모박스","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHcCl8G2pIAYWJnTlHiJMoKULVcpaM5oyN90l4iQ=s64","userId":"03122266989642426519"}},"outputId":"a44b75a0-5e59-403f-d27c-0d842f025c51","colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["# 그래프를 실행할 세션 구성\n","sess = tf.Session()\n","# sess.run: 설정한 텐서 그래프(변수나 수식 등등)를 실행합니다.\n","print(sess.run(hello))\n","print(sess.run([a, b, c]))\n","\n","# 세션을 닫습니다.\n","sess.close()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["b'Hello, TensorFlow!'\n","[10, 32, 42]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"R806J32bFHAn"},"source":["# placeholder, variable"]},{"cell_type":"markdown","metadata":{"id":"0ehjhicOXaFV"},"source":["placeholder: 계산에서 입력값을 받는 변수들  \n","Variable : 계산하면서 최적화 할 변수들"]},{"cell_type":"code","metadata":{"id":"YeaCeY4TE8OW","executionInfo":{"status":"ok","timestamp":1603201223718,"user_tz":-540,"elapsed":3029,"user":{"displayName":"네모박스","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHcCl8G2pIAYWJnTlHiJMoKULVcpaM5oyN90l4iQ=s64","userId":"03122266989642426519"}},"outputId":"47aef55f-818b-4d0f-a7fe-f0e35733d5f5","colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["# tf.placeholder: 계산을 실행할 때 입력값을 받는 변수로 사용합니다.\n","# None 은 크기가 정해지지 않았음을 의미합니다.\n","X = tf.placeholder(tf.float32, [None, 3])\n","print(X)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Tensor(\"Placeholder:0\", shape=(?, 3), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"13GzzAQoHZzq"},"source":["그런데, 막상 실행해보면, a*b+c의 값이 아니라 다음과 같이 Tensor… 라는 문자열이 출력된다. 실제로 값을 뽑아내려면, 이 정의된 그래프에 a,b,c 값을 넣어서 실행해야 하는데, 세션 (Session)을 생성해야 한다."]},{"cell_type":"code","metadata":{"id":"9T68N_D0Faa_","executionInfo":{"status":"ok","timestamp":1603201223719,"user_tz":-540,"elapsed":3027,"user":{"displayName":"네모박스","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHcCl8G2pIAYWJnTlHiJMoKULVcpaM5oyN90l4iQ=s64","userId":"03122266989642426519"}}},"source":["# X 플레이스홀더에 넣을 값 입니다.\n","# 플레이스홀더에서 설정한 것 처럼, 두번째 차원의 요소의 갯수는 3개 입니다.\n","x_data = [[1, 2, 3], [4, 5, 6]]"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"JGOWC0CEFk73","executionInfo":{"status":"ok","timestamp":1603201224007,"user_tz":-540,"elapsed":3313,"user":{"displayName":"네모박스","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHcCl8G2pIAYWJnTlHiJMoKULVcpaM5oyN90l4iQ=s64","userId":"03122266989642426519"}}},"source":["# tf.Variable: 그래프를 계산하면서 최적화 할 변수들입니다. 이 값이 바로 신경망을 좌우하는 값들입니다.\n","# tf.random_normal: 각 변수들의 초기값을 정규분포 랜덤 값으로 초기화합니다.\n","W = tf.Variable(tf.random_normal([3, 2]))\n","b = tf.Variable(tf.random_normal([2, 1]))"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"301mQDHVFp6D","executionInfo":{"status":"ok","timestamp":1603201224007,"user_tz":-540,"elapsed":3311,"user":{"displayName":"네모박스","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHcCl8G2pIAYWJnTlHiJMoKULVcpaM5oyN90l4iQ=s64","userId":"03122266989642426519"}}},"source":["# 입력값과 변수들을 계산할 수식을 작성합니다.\n","# tf.matmul 처럼 mat* 로 되어 있는 함수로 행렬 계산을 수행합니다.\n","# 아직 X에는 어떤 값도 넣지 않았습니다. \n","expr = tf.matmul(X, W) + b"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"KRWQBm28FtpT","executionInfo":{"status":"ok","timestamp":1603201224008,"user_tz":-540,"elapsed":3310,"user":{"displayName":"네모박스","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHcCl8G2pIAYWJnTlHiJMoKULVcpaM5oyN90l4iQ=s64","userId":"03122266989642426519"}}},"source":["# 세션을 열고, 위에서 설정한 Variable 들의 값들을 초기화 하기 위해\n","# 처음에 tf.global_variables_initializer 를 한 번 실행해야 합니다.\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"hjV3RFSWF5yL","executionInfo":{"status":"ok","timestamp":1603201224008,"user_tz":-540,"elapsed":3304,"user":{"displayName":"네모박스","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHcCl8G2pIAYWJnTlHiJMoKULVcpaM5oyN90l4iQ=s64","userId":"03122266989642426519"}},"outputId":"364b16d1-de56-4859-e5b3-eea14c7519ae","colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["print(\"=== x_data ===\")\n","print(x_data)\n","print(\"=== W ===\")\n","print(sess.run(W))\n","print(\"=== b ===\")\n","print(sess.run(b))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["=== x_data ===\n","[[1, 2, 3], [4, 5, 6]]\n","=== W ===\n","[[ 0.2136517  -1.5314494 ]\n"," [-0.0714187  -0.30341563]\n"," [-1.2406017   0.32053345]]\n","=== b ===\n","[[1.405224 ]\n"," [0.5947467]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NCiTH5AbGFMd","executionInfo":{"status":"ok","timestamp":1603201224009,"user_tz":-540,"elapsed":3300,"user":{"displayName":"네모박스","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHcCl8G2pIAYWJnTlHiJMoKULVcpaM5oyN90l4iQ=s64","userId":"03122266989642426519"}},"outputId":"e87b770d-d6e1-406c-8d7e-d1841ff4d532","colab":{"base_uri":"https://localhost:8080/","height":73}},"source":["print(\"=== expr ===\")\n","# expr 수식에는 X 라는 '입력값'이 필요합니다.\n","# 변수나 함수처럼 입력값이 필요한 대상을 run 하기 위해서는 feed_dict를 통해 값을 입력합니다.\n","# 따라서 expr 실행시에는 이 변수에 대한 실제 입력값을 다음처럼 넣어줘야합니다.\n","# 딥러닝을 진행하다보면 엄청나게 많은 학습을 하는 도중에 값이 수시로 업데이트되는데,\n","# 학습을 하는 과정에서 최적화되며 유동적인 값이 되는 값을 feed_dict로 넣어줍니다.\n","print(sess.run(expr, feed_dict={X: x_data})) # expr 안에 X를 입력할 건데, X에 는 x_data가 들어갑니다.\n","\n","sess.close()"],"execution_count":12,"outputs":[{"output_type":"stream","text":["=== expr ===\n","[[-2.2457666   0.22854364]\n"," [-6.3513503  -5.1249285 ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YdxOzEVYRYd8"},"source":["주의. `tf.placeholder`로 정의한 변수는 반드시 `feed_dict` option (dictionary 타입)으로 지정해야 합니다. 그렇지 않을 경우 Invalid Argument Error"]},{"cell_type":"markdown","metadata":{"id":"nNCU5VCUIv-3"},"source":["# Linear Regresssion\n","\n","X 와 Y 의 상관관계를 분석하는 기초적인 선형 회귀 모델을 만들고 실행해봅니다.\n"]},{"cell_type":"code","metadata":{"id":"ncN99GjrIyMx","executionInfo":{"status":"ok","timestamp":1603201224009,"user_tz":-540,"elapsed":3298,"user":{"displayName":"네모박스","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHcCl8G2pIAYWJnTlHiJMoKULVcpaM5oyN90l4iQ=s64","userId":"03122266989642426519"}}},"source":["x_data = [1, 2, 3]\n","y_data = [1, 2, 3]\n","\n","# tf.random_uniform \n","# 정규분포 난수를 생성하는 함수.\n","# 배열의 shape, 최소값, 최대값을 파라미터로 사용합니다.\n","# 여기서는 [1], -1.0, 1.0을 전달했기 때문에 -1에서 1 사이의 난수를 1개 만듭니다. \n","# 결과는 1행 1열의 행렬이 됩니다.\n","W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n","b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"bPfAMsGiJaoS","executionInfo":{"status":"ok","timestamp":1603201224009,"user_tz":-540,"elapsed":3293,"user":{"displayName":"네모박스","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHcCl8G2pIAYWJnTlHiJMoKULVcpaM5oyN90l4iQ=s64","userId":"03122266989642426519"}},"outputId":"cdc3a7b7-24e6-4795-9a5a-737a673a6d5b","colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["# name: 나중에 (텐서보드등으로) 값의 변화를 추적하거나 살펴보기 쉽게 하기 위해 이름을 붙일 수도 있습니다.\n","X = tf.placeholder(tf.float32, name=\"X\")\n","Y = tf.placeholder(tf.float32, name=\"Y\")\n","print(X)  # X_런타임횟수:0  Y_런타임횟수:0로 찍힙니다.. 아직은 신경쓸 필요 없습니다\n","print(Y)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Tensor(\"X:0\", dtype=float32)\n","Tensor(\"Y:0\", dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qD80mK59XuDo"},"source":["최적화할 변수 : W, b  \n","입력값으로 사용할 변수 : X, Y"]},{"cell_type":"code","metadata":{"id":"NZpxo2N-J7RV","executionInfo":{"status":"ok","timestamp":1603201224010,"user_tz":-540,"elapsed":3293,"user":{"displayName":"네모박스","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHcCl8G2pIAYWJnTlHiJMoKULVcpaM5oyN90l4iQ=s64","userId":"03122266989642426519"}}},"source":["# X 와 Y 의 상관 관계를 분석하기 위한 가설 수식을 작성합니다.\n","# y = W * x + b\n","# W 와 X 가 행렬이 아니므로 tf.matmul 이 아니라 기본 곱셈 기호를 사용했습니다.\n","hypothesis = W * X + b"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"uGLIow7_KDDh","executionInfo":{"status":"ok","timestamp":1603201224010,"user_tz":-540,"elapsed":3291,"user":{"displayName":"네모박스","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHcCl8G2pIAYWJnTlHiJMoKULVcpaM5oyN90l4iQ=s64","userId":"03122266989642426519"}}},"source":["# 손실(오차) 함수를 작성합니다. 여기서는 Mean Squared Error function으로 작성합니다.\n","# mean(h - Y)^2 : 예측값과 실제값의 거리를 비용(손실) 함수로 정합니다.\n","cost = tf.reduce_mean(tf.square(hypothesis - Y)) # 우리의 목표는 이 cost를 최소화하는 것입니다."],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"AG_PrVXCKIc5","executionInfo":{"status":"ok","timestamp":1603201224011,"user_tz":-540,"elapsed":3290,"user":{"displayName":"네모박스","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHcCl8G2pIAYWJnTlHiJMoKULVcpaM5oyN90l4iQ=s64","userId":"03122266989642426519"}}},"source":["# 텐서플로우에 기본적으로 포함되어 있는 함수를 이용해 경사 하강법 최적화를 수행합니다.\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n","# 비용을 최소화 하는 것이 최종 목표. 어떤 수단으로? GradientDescentOptimizer를 minimize하는 방식으로.\n","train_op = optimizer.minimize(cost)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NbO8wLbRP2eM"},"source":["자, 모델의 정의가 끝났으면 이제 training을 시작해봅시다.\n","\n","- 우리가 원하는 것은? : cost를 최소화하는 것\n","- cost를 최소화하기 위해 선택한 건? : GradientDescentOptimizer를 minimize하는 것"]},{"cell_type":"code","metadata":{"id":"_78jmtAhKK2Z","executionInfo":{"status":"ok","timestamp":1603201224563,"user_tz":-540,"elapsed":3837,"user":{"displayName":"네모박스","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHcCl8G2pIAYWJnTlHiJMoKULVcpaM5oyN90l4iQ=s64","userId":"03122266989642426519"}},"outputId":"c52eedf5-b820-40c0-e012-fbfe1f488780","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# 세션을 생성하고 초기화합니다.\n","with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","\n","    # 최적화를 100번 수행합니다.\n","    for step in range(100):\n","        \n","        # sess.run 을 통해 train_op 와 cost를 계산합니다. 앞의 예제에서는  하나의 노드만 가져왔지만 복수의 tensor를 받아올 수도 있습니다. (어차피 병렬로 계산됨)\n","        # 이 때, 가설 수식에 넣어야 할 실제값(Placeholder)을 feed_dict 을 통해 전달합니다.\n","        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y: y_data})  # train_op 값에는 관심 없음\n","              # underscore in python? : https://mingrammer.com/underscore-in-python/\n","        print(step, \"번째 학습\\t\", \"손실(오차)\", cost_val, \" weight:\",sess.run(W), \" bias:\",sess.run(b))\n","        \n","    # 최적화가 완료된 모델에 테스트 값을 넣고 결과가 잘 나오는지 확인해봅니다.\n","    print(\"\\n=== Test 결과 ===\")\n","    print(\"X: 5, Y:\", sess.run(hypothesis, feed_dict={X: 5}))       # X가 5일 때 Y 값 (W * X + b)\n","    print(\"X: 2.5, Y:\", sess.run(hypothesis, feed_dict={X: 2.5}))   # X가 2.5일 때 Y 값 (W * X + b)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["0 번째 학습\t 손실(오차) 11.568146  weight: [0.55935764]  bias: [1.3891613]\n","1 번째 학습\t 손실(오차) 0.38738248  weight: [0.4149593]  bias: [1.287586]\n","2 번째 학습\t 손실(오차) 0.24198903  weight: [0.44596288]  bias: [1.264085]\n","3 번째 학습\t 손실(오차) 0.22897752  weight: [0.45743018]  bias: [1.2328829]\n","4 번째 학습\t 손실(오차) 0.21808271  weight: [0.4706755]  bias: [1.2033342]\n","5 번째 학습\t 손실(오차) 0.20772342  weight: [0.483378]  bias: [1.1743971]\n","6 번째 학습\t 손실(오차) 0.19785638  weight: [0.49579966]  bias: [1.1461664]\n","7 번째 학습\t 손실(오차) 0.18845809  weight: [0.50792]  bias: [1.1186132]\n","8 번째 학습\t 손실(오차) 0.17950617  weight: [0.51974934]  bias: [1.0917226]\n","9 번째 학습\t 손실(오차) 0.17097951  weight: [0.5312943]  bias: [1.0654783]\n","10 번째 학습\t 손실(오차) 0.16285788  weight: [0.5425616]  bias: [1.0398649]\n","11 번째 학습\t 손실(오차) 0.15512198  weight: [0.5535582]  bias: [1.0148673]\n","12 번째 학습\t 손실(오차) 0.14775349  weight: [0.5642903]  bias: [0.9904706]\n","13 번째 학습\t 손실(오차) 0.14073516  weight: [0.57476443]  bias: [0.9666603]\n","14 번째 학습\t 손실(오차) 0.13405015  weight: [0.58498687]  bias: [0.9434225]\n","15 번째 학습\t 손실(오차) 0.12768264  weight: [0.5949635]  bias: [0.9207432]\n","16 번째 학습\t 손실(오차) 0.12161764  weight: [0.6047003]  bias: [0.89860916]\n","17 번째 학습\t 손실(오차) 0.11584067  weight: [0.61420304]  bias: [0.8770072]\n","18 번째 학습\t 손실(오차) 0.11033812  weight: [0.62347734]  bias: [0.85592455]\n","19 번째 학습\t 손실(오차) 0.105097  weight: [0.63252866]  bias: [0.8353487]\n","20 번째 학습\t 손실(오차) 0.10010484  weight: [0.6413624]  bias: [0.8152675]\n","21 번째 학습\t 손실(오차) 0.09534979  weight: [0.6499838]  bias: [0.795669]\n","22 번째 학습\t 손실(오차) 0.09082059  weight: [0.658398]  bias: [0.7765417]\n","23 번째 학습\t 손실(오차) 0.086506516  weight: [0.6666099]  bias: [0.7578742]\n","24 번째 학습\t 손실(오차) 0.082397416  weight: [0.6746243]  bias: [0.7396554]\n","25 번째 학습\t 손실(오차) 0.07848347  weight: [0.6824462]  bias: [0.7218746]\n","26 번째 학습\t 손실(오차) 0.07475543  weight: [0.6900799]  bias: [0.7045212]\n","27 번째 학습\t 손실(오차) 0.071204506  weight: [0.6975302]  bias: [0.687585]\n","28 번째 학습\t 손실(오차) 0.06782222  weight: [0.7048013]  bias: [0.6710559]\n","29 번째 학습\t 손실(오차) 0.06460062  weight: [0.71189773]  bias: [0.6549242]\n","30 번째 학습\t 손실(오차) 0.0615321  weight: [0.71882355]  bias: [0.6391803]\n","31 번째 학습\t 손실(오차) 0.05860926  weight: [0.7255828]  bias: [0.6238148]\n","32 번째 학습\t 손실(오차) 0.05582528  weight: [0.73217964]  bias: [0.60881877]\n","33 번째 학습\t 손실(오차) 0.053173523  weight: [0.7386178]  bias: [0.59418315]\n","34 번째 학습\t 손실(오차) 0.050647736  weight: [0.74490124]  bias: [0.57989943]\n","35 번째 학습\t 손실(오차) 0.048241932  weight: [0.7510336]  bias: [0.56595904]\n","36 번째 학습\t 손실(오차) 0.045950416  weight: [0.7570186]  bias: [0.5523538]\n","37 번째 학습\t 손실(오차) 0.04376772  weight: [0.7628597]  bias: [0.5390756]\n","38 번째 학습\t 손실(오차) 0.041688755  weight: [0.7685604]  bias: [0.5261166]\n","39 번째 학습\t 손실(오차) 0.039708495  weight: [0.774124]  bias: [0.5134691]\n","40 번째 학습\t 손실(오차) 0.037822302  weight: [0.77955395]  bias: [0.5011257]\n","41 번째 학습\t 손실(오차) 0.036025744  weight: [0.78485334]  bias: [0.48907897]\n","42 번째 학습\t 손실(오차) 0.034314495  weight: [0.7900253]  bias: [0.47732183]\n","43 번째 학습\t 손실(오차) 0.032684494  weight: [0.795073]  bias: [0.46584734]\n","44 번째 학습\t 손실(오차) 0.031131983  weight: [0.7999993]  bias: [0.4546487]\n","45 번째 학습\t 손실(오차) 0.029653167  weight: [0.8048071]  bias: [0.44371924]\n","46 번째 학습\t 손실(오차) 0.028244644  weight: [0.8094995]  bias: [0.43305254]\n","47 번째 학습\t 손실(오차) 0.026902989  weight: [0.8140789]  bias: [0.42264223]\n","48 번째 학습\t 손실(오차) 0.02562507  weight: [0.8185483]  bias: [0.4124822]\n","49 번째 학습\t 손실(오차) 0.024407849  weight: [0.8229103]  bias: [0.40256643]\n","50 번째 학습\t 손실(오차) 0.023248479  weight: [0.82716745]  bias: [0.39288902]\n","51 번째 학습\t 손실(오차) 0.022144176  weight: [0.8313222]  bias: [0.38344422]\n","52 번째 학습\t 손실(오차) 0.021092286  weight: [0.8353771]  bias: [0.37422648]\n","53 번째 학습\t 손실(오차) 0.020090396  weight: [0.83933455]  bias: [0.36523035]\n","54 번째 학습\t 손실(오차) 0.019136092  weight: [0.8431968]  bias: [0.35645044]\n","55 번째 학습\t 손실(오차) 0.018227113  weight: [0.8469663]  bias: [0.34788164]\n","56 번째 학습\t 손실(오차) 0.017361304  weight: [0.8506451]  bias: [0.3395188]\n","57 번째 학습\t 손실(오차) 0.016536621  weight: [0.8542355]  bias: [0.33135697]\n","58 번째 학습\t 손실(오차) 0.015751116  weight: [0.85773957]  bias: [0.32339138]\n","59 번째 학습\t 손실(오차) 0.015002918  weight: [0.8611594]  bias: [0.31561726]\n","60 번째 학습\t 손실(오차) 0.014290277  weight: [0.86449707]  bias: [0.30803004]\n","61 번째 학습\t 손실(오차) 0.013611485  weight: [0.86775446]  bias: [0.3006252]\n","62 번째 학습\t 손실(오차) 0.012964926  weight: [0.87093353]  bias: [0.29339835]\n","63 번째 학습\t 손실(오차) 0.012349087  weight: [0.8740362]  bias: [0.28634527]\n","64 번째 학습\t 손실(오차) 0.011762501  weight: [0.87706435]  bias: [0.27946174]\n","65 번째 학습\t 손실(오차) 0.011203751  weight: [0.88001955]  bias: [0.27274364]\n","66 번째 학습\t 손실(오차) 0.010671578  weight: [0.8829039]  bias: [0.2661871]\n","67 번째 학습\t 손실(오차) 0.010164666  weight: [0.8857187]  bias: [0.25978813]\n","68 번째 학습\t 손실(오차) 0.009681856  weight: [0.88846606]  bias: [0.25354305]\n","69 번째 학습\t 손실(오차) 0.00922195  weight: [0.8911472]  bias: [0.24744801]\n","70 번째 학습\t 손실(오차) 0.008783901  weight: [0.8937639]  bias: [0.24149953]\n","71 번째 학습\t 손실(오차) 0.0083666695  weight: [0.89631784]  bias: [0.23569408]\n","72 번째 학습\t 손실(오차) 0.007969236  weight: [0.8988102]  bias: [0.23002812]\n","73 번째 학습\t 손실(오차) 0.0075906827  weight: [0.90124273]  bias: [0.22449842]\n","74 번째 학습\t 손실(오차) 0.007230124  weight: [0.9036168]  bias: [0.21910164]\n","75 번째 학습\t 손실(오차) 0.0068866997  weight: [0.9059338]  bias: [0.2138346]\n","76 번째 학습\t 손실(오차) 0.0065595754  weight: [0.9081951]  bias: [0.20869416]\n","77 번째 학습\t 손실(오차) 0.006247979  weight: [0.910402]  bias: [0.2036773]\n","78 번째 학습\t 손실(오차) 0.005951205  weight: [0.91255593]  bias: [0.19878106]\n","79 번째 학습\t 손실(오차) 0.005668513  weight: [0.914658]  bias: [0.19400248]\n","80 번째 학습\t 손실(오차) 0.00539926  weight: [0.91670954]  bias: [0.18933879]\n","81 번째 학습\t 손실(오차) 0.0051427865  weight: [0.9187118]  bias: [0.18478721]\n","82 번째 학습\t 손실(오차) 0.0048984913  weight: [0.92066586]  bias: [0.18034504]\n","83 번째 학습\t 손실(오차) 0.0046658157  weight: [0.92257303]  bias: [0.17600968]\n","84 번째 학습\t 손실(오차) 0.0044441964  weight: [0.9244343]  bias: [0.17177853]\n","85 번째 학습\t 손실(오차) 0.004233087  weight: [0.9262509]  bias: [0.1676491]\n","86 번째 학습\t 손실(오차) 0.004032017  weight: [0.92802376]  bias: [0.16361894]\n","87 번째 학습\t 손실(오차) 0.00384049  weight: [0.929754]  bias: [0.15968564]\n","88 번째 학습\t 손실(오차) 0.0036580611  weight: [0.9314427]  bias: [0.15584691]\n","89 번째 학습\t 손실(오차) 0.0034843062  weight: [0.93309075]  bias: [0.15210046]\n","90 번째 학습\t 손실(오차) 0.0033187922  weight: [0.9346992]  bias: [0.14844406]\n","91 번째 학습\t 손실(오차) 0.003161141  weight: [0.936269]  bias: [0.14487557]\n","92 번째 학습\t 손실(오차) 0.0030109978  weight: [0.93780106]  bias: [0.14139286]\n","93 번째 학습\t 손실(오차) 0.002867968  weight: [0.93929625]  bias: [0.13799386]\n","94 번째 학습\t 손실(오차) 0.002731736  weight: [0.94075555]  bias: [0.13467659]\n","95 번째 학습\t 손실(오차) 0.0026019786  weight: [0.94217974]  bias: [0.13143905]\n","96 번째 학습\t 손실(오차) 0.0024783819  weight: [0.9435697]  bias: [0.12827934]\n","97 번째 학습\t 손실(오차) 0.002360654  weight: [0.9449262]  bias: [0.12519558]\n","98 번째 학습\t 손실(오차) 0.0022485272  weight: [0.9462502]  bias: [0.12218599]\n","99 번째 학습\t 손실(오차) 0.0021417162  weight: [0.9475423]  bias: [0.11924872]\n","\n","=== Test 결과 ===\n","X: 5, Y: [4.8569603]\n","X: 2.5, Y: [2.4881043]\n"],"name":"stdout"}]}]}